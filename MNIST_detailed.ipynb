{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (X_test, Y_test) = mnist.load_data()\n",
    "print(X_test.shape)\n",
    "# print(Y_test)\n",
    "Y_test = Y_test[:, np.newaxis] # Creates a new axis and then takes all the elements and puts them along that axis\n",
    "# print(Y_test)\n",
    "\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "# print(X_test[0])\n",
    "# print(X_test.shape)\n",
    "test_data = np.concatenate((Y_test, X_test), axis=1)\n",
    "# print(test_data.shape)\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "# test_data = pd.read_csv('test.csv')\n",
    "data = np.array(data)\n",
    "# test_data = np.array(test_data)\n",
    "print(data[0])\n",
    "# print(test_data.shape)\n",
    "\n",
    "# This is the shape of the data, we have 42,000 images and an array of 785 values\n",
    "# for each of these images (label in the first column, 2-785 are the pixel values)\n",
    "print(data.shape)\n",
    "m, n = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.  6.6 7.7]\n",
      " [3.  4.4 5.5]\n",
      " [1.  2.2 3.3]]\n"
     ]
    }
   ],
   "source": [
    "# Now we shuffle the data (example below)\n",
    "test_arr = np.array([[1,2.2,3.3], [3,4.4,5.5], [5,6.6,7.7]])\n",
    "np.random.shuffle(test_arr)\n",
    "print(test_arr)\n",
    "\n",
    "# Show we shuffle all the images to throw into the model (reduces bias and introduces independence)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.  3.  1. ]\n",
      " [6.6 4.4 2.2]\n",
      " [7.7 5.5 3.3]]\n",
      "(785, 42000)\n",
      "(785, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Now we transpose the array (example)\n",
    "t_test_array = test_arr.T\n",
    "print(t_test_array)\n",
    "\n",
    "# We want to transpose the entire x data (first we are slicing an array from the start\n",
    "# to the end at 46,000 that generates a new arry). Then transpose so we can pass into neural network \n",
    "# (because imput layer is vertical):\n",
    "data_train = data[0:m].T\n",
    "data_test = test_data[0:m].T\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(42000,)\n",
      "(784, 42000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# After the transpose, the first row of the data is going to be the labels (which is your y)\n",
    "# Example \n",
    "y_label_test_array = t_test_array[0]\n",
    "x_label_test_array = t_test_array[1:]\n",
    "\n",
    "# print(y_label_test_array)\n",
    "# print(x_label_test_array)\n",
    "\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "Y_test = data_test[0]\n",
    "X_test = data_train[1:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n"
     ]
    }
   ],
   "source": [
    "# We divide the training data to normalize the inputs to fall into the 0 to 1 range (grayscaling)\n",
    "# The \".\"\" here makes sure we do float division \n",
    "X_train = X_train / 255.\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(10, 784) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    # Initialize weights and biases for a neural network with one hidden layer.\n",
    "    # W1 is the weight matrix for the connections between the input layer and the hidden layer.\n",
    "    # b1 is the bias vector for the hidden layer.\n",
    "    # W2 is the weight matrix for the connections between the hidden layer and the output layer.\n",
    "    # b2 is the bias vector for the output layer.\n",
    "\n",
    "    # We want to include the bias terms here to make sure the model learns and the does not get a bunch of zeros \n",
    "\n",
    "    # Here we use np.random.rand to generate random numbers between 0 and 1. (if we leave at this value the outputs of the neurons can become \n",
    "    # very small or very large which would I turn lead to the the gradient becoming very small or the \"vanishing gradient problem\")\n",
    "    \n",
    "    # By subtracting 0.5, we shift the range to be between -0.5 and 0.5. (convergence during training as weights initialized close to zero\n",
    "    # lead to faster convergence).\n",
    "\n",
    "    # The input layer has 784 neurons (for 28x28 pixel images, 784 = 28 * 28),\n",
    "    # and the hidden layer has 10 neurons (can be changed).\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "\n",
    "    # The bias vector for the hidden layer has 10 elements, one for each neuron in the hidden layer.\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    # The output layer has 10 neurons (one for each class in a digit classification task).\n",
    "    # W2 connects 10 neurons in the hidden layer to 10 neurons in the output layer.\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "\n",
    "    # The bias vector for the output layer also has 10 elements.\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(Z):\n",
    "    # Input is going to be the activation after the weight dot product is taken and then bias is added \n",
    "    # ReLU stands for Rectified linear Unit \n",
    "    # Helps us learn non-linear features within the models, the ReLU function introduces non-linearity \n",
    "    # The return values is the output of the neurons in the hidden layer\n",
    "    # Sets negative values to zero and then keeps all the positive values\n",
    "    # np.maximum compares two arrays element wise and returns the maximum  \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    # The derivative of ReLU\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "    # Softmax computes the raw scores of the nerons into probabilites that all sum up to 1\n",
    "    # Softmax is helpful in multiclass classification problems where the network needs to know the probabilites of each class\n",
    "    # This is calculated by computing the exponential score of each element and then dividing by the sum of all expoonents (normalizing)\n",
    "    # print(Z.shape) # (10, 42000)\n",
    "    # return 0\n",
    "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0) # Using axis = 0 here to sum the (based of the equation we are summing up the classes)\n",
    "    return A \n",
    "\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    # print(W1.shape)\n",
    "    # print(X.shape)\n",
    "    # Now we do the linear transformation (the combines the weight and input tensor using the dot product and the weight)\n",
    "    # The connecting part of the neurons is done by matrix multiplication (Weight tensor x X tensor...)\n",
    "    Z1 = W1.dot(X) + b1 #Z1 is the activation here\n",
    "    # print(Z1[0])\n",
    "    # ReLU helps us get the output of the of the neurons within the hidden layer \n",
    "    A1 = ReLu(Z1)\n",
    "    # print(A1[0])\n",
    "    # Now, we have to connect the neurons from the hidden layer to the output layer, this is done by taking another dot product and adding bias\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    # Now, we get the output of neurons in the output layer, since we want probablilies, we use the softmax equations (due to this being a classification problem)\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    # One hot encoding tranforms the labels into a binary format that the network can use for calculating the loss (many loss functions need this)\n",
    "    # First step is to create a matrix of with dimenstions (num_of_samples, number_of_classes)\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # We use Y.max here because this will return 9 + 1 which is 10 total class for 0-9 digits\n",
    "    # Now, for each sample, we are setting the element corresponding to its class to one\n",
    "    # This generates an row indicies in an array of Y.size so like [0, 1,....41998, 41999].\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1 # This sets \n",
    "    # print(f'One hot array shape transpoded: {one_hot_Y.T.shape}')\n",
    "    return one_hot_Y.T\n",
    "\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m = X.shape[1]\n",
    "    # Back propagation is a key algorithm that helps us update the weights and biases of the NN by minimzing the loss function \n",
    "    # Primary goal is to minimize the loss function (do this by seeing how well the predictions match the actual label)\n",
    "    # Back Prop does gradient descent to update the weights and bias (which allows us to update the params to reduce the loss)\n",
    "    # propigates the error back through the network using the chain rule (gradient of the computed loss)\n",
    "\n",
    "    # Back prop over the softmax and compute the gradient of the output layer\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    # Gradient of cross entropy loss with respect to Z2 \n",
    "    # Next, we subtract the one_hot_y from A2. If the predicted probability is high, then the loss will be low  \n",
    "    dZ2 = A2 - one_hot_Y\n",
    "\n",
    "    # The next step is to fine how W2 effects the lost. This is computed using the chain rule and is the derivative of the \n",
    "    # loss and the activations of the hidden layer. \n",
    "    dW2 = (1 / m) * dZ2.dot(A1.T)\n",
    "\n",
    "    # The bias is the weighted sum of the inputs to the output layer. Sum all the gradients of the loss with resepect to Z2\n",
    "    db2 = (1 / m) * np.sum(dZ2)\n",
    "\n",
    "\n",
    "    # Now we have to backprop over the ReLU layer\n",
    "    dA1 = W2.T.dot(dZ2)\n",
    "    dZ1 = dA1 * deriv_ReLU(Z1) \n",
    "\n",
    "    dW1 = (1 / m) * dZ1.dot(X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1)\n",
    "\n",
    "    return dW1, db1, dW2, db2 \n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    # Updating the weights in th opposite direction of the gradient of the loss \n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_arr = np.array([0,4,2,1,3])\n",
    "# Y_train_arr_one_hot = one_hot(Y_train_arr)\n",
    "# print(Y_train_arr)\n",
    "# print(Y_train_arr_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = init_params()\n",
    "Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_train)\n",
    "dW1, db1, dW2, db2  = back_prop(Z1, A1, Z2, A2, W1, W2, X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to get the predictions (getting the class with the highest probability within the output tensor from the output layer)\n",
    "def get_predictions(A2):\n",
    "    A2_max = np.argmax(A2, axis=0)\n",
    "    return A2_max\n",
    "\n",
    "\n",
    "# Next is to get the accuarcy of your predictions compared to the actual labels \n",
    "def get_accuracy(predictions, Y):\n",
    "    num = np.sum(predictions == Y)\n",
    "    return num / Y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now we are actually running the gradient descent algo:\n",
    "def gradient_descent(X_train, Y_train, X_test, Y_test, learning_rate, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_train)\n",
    "        dW1, db1, dW2, db2  = back_prop(Z1, A1, Z2, A2, W1, W2, X_train, Y_train)\n",
    "\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            predictions = get_predictions(A2)\n",
    "            train_accuracy = get_accuracy(predictions, Y_train)\n",
    "            print(f'Iteration: {i + 1}, Train Accuarcy: {train_accuracy * 100:.2f}%')\n",
    "\n",
    "            # _, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)\n",
    "\n",
    "            # predictions_test = get_predictions(A2_test)\n",
    "            #  # Debug print statements\n",
    "            # print(f\"Shape of predictions: {predictions_test.shape}\")\n",
    "            # print(f\"Shape of Y_test: {Y_test.shape}\")\n",
    "            # train_accuracy = get_accuracy(predictions_test, Y_test)\n",
    "            # print(f'Iteration: {i + 1}, Train Accuarcy: {train_accuracy * 100:.2f}%')\n",
    "\n",
    "    return W1, b1, W2, b2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50, Train Accuarcy: 40.97%\n",
      "Iteration: 100, Train Accuarcy: 63.72%\n",
      "Iteration: 150, Train Accuarcy: 71.78%\n",
      "Iteration: 200, Train Accuarcy: 76.26%\n",
      "Iteration: 250, Train Accuarcy: 78.94%\n",
      "Iteration: 300, Train Accuarcy: 80.78%\n",
      "Iteration: 350, Train Accuarcy: 82.14%\n",
      "Iteration: 400, Train Accuarcy: 83.22%\n",
      "Iteration: 450, Train Accuarcy: 84.11%\n",
      "Iteration: 500, Train Accuarcy: 84.75%\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, X_test, Y_test, 0.01, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
